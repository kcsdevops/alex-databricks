# Databricks notebook source
# MAGIC %md
# MAGIC # ğŸš€ Getting Started with Azure Databricks
# MAGIC 
# MAGIC **Bem-vindo ao seu ambiente de desenvolvimento Databricks!**
# MAGIC 
# MAGIC Este notebook serve como introduÃ§Ã£o ao seu novo ambiente Analytics otimizado para custos.
# MAGIC 
# MAGIC ## ğŸ“‹ InformaÃ§Ãµes do Ambiente
# MAGIC 
# MAGIC - **Cluster**: Auto-terminaÃ§Ã£o em 20 minutos
# MAGIC - **InstÃ¢ncias**: Spot instances para economia de atÃ© 80%
# MAGIC - **Auto-scaling**: 1-2 workers conforme demanda
# MAGIC - **Custo estimado**: ~R$ 71.23/mÃªs
# MAGIC 
# MAGIC ## ğŸ¯ PrÃ³ximos Passos
# MAGIC 
# MAGIC 1. âœ… Execute este notebook para verificar a configuraÃ§Ã£o
# MAGIC 2. ğŸ“Š Explore os notebooks de anÃ¡lise de dados
# MAGIC 3. ğŸ¤– Teste os exemplos de Machine Learning
# MAGIC 4. ğŸ“ˆ Configure monitoramento de custos

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”§ VerificaÃ§Ã£o do Ambiente
# MAGIC 
# MAGIC Vamos verificar se tudo estÃ¡ funcionando corretamente:

# COMMAND ----------

# Verificar versÃ£o do Spark
print(f"ğŸ”¥ Spark Version: {spark.version}")
print(f"ğŸ Python Version: {sys.version}")

# Verificar configuraÃ§Ã£o do cluster
print(f"ğŸ–¥ï¸  Cluster Name: {spark.conf.get('spark.databricks.clusterUsageTags.clusterName')}")
print(f"âš¡ Driver Node Type: {spark.conf.get('spark.databricks.clusterUsageTags.clusterNodeType')}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Teste de Funcionalidade BÃ¡sica
# MAGIC 
# MAGIC Vamos criar um DataFrame simples para testar as funcionalidades:

# COMMAND ----------

import sys
from datetime import datetime
from pyspark.sql import functions as F
from pyspark.sql.types import *

# Criar dados de exemplo
data = [
    ("Azure", "Databricks", 100, "2025-01-01"),
    ("Machine", "Learning", 200, "2025-01-02"),
    ("Data", "Analytics", 150, "2025-01-03"),
    ("Cost", "Optimization", 80, "2025-01-04"),
    ("Spot", "Instances", 60, "2025-01-05")
]

schema = StructType([
    StructField("category", StringType(), True),
    StructField("subcategory", StringType(), True),
    StructField("value", IntegerType(), True),
    StructField("date", StringType(), True)
])

# Criar DataFrame
df = spark.createDataFrame(data, schema)

# Mostrar dados
print("ğŸ“‹ Dados de Exemplo:")
df.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ˆ AnÃ¡lise BÃ¡sica dos Dados

# COMMAND ----------

# EstatÃ­sticas descritivas
print("ğŸ“Š EstatÃ­sticas Descritivas:")
df.describe().show()

# AgregaÃ§Ãµes simples
print("ğŸ”¢ Soma total por categoria:")
df.groupBy("category").agg(F.sum("value").alias("total_value")).orderBy(F.desc("total_value")).show()

# Criar visualizaÃ§Ã£o simples
display(df.groupBy("category").agg(F.sum("value").alias("total_value")).orderBy(F.desc("total_value")))

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’° InformaÃ§Ãµes de Custo e OtimizaÃ§Ã£o

# COMMAND ----------

# InformaÃ§Ãµes sobre otimizaÃ§Ã£o de custos
cost_info = {
    "cluster_config": {
        "auto_termination": "20 minutos",
        "instance_type": "Standard_DS3_v2",  
        "spot_instances": "Habilitado",
        "auto_scaling": "1-2 workers"
    },
    "estimated_costs": {
        "monthly_usd": 13.39,
        "monthly_brl": 71.23,
        "savings_with_optimization": "75-88%"
    },
    "cost_controls": [
        "Auto-terminaÃ§Ã£o de cluster",
        "Uso de Spot Instances",
        "Auto-scaling responsivo",
        "Monitoramento via Azure Cost Management"
    ]
}

print("ğŸ’° ConfiguraÃ§Ã£o de OtimizaÃ§Ã£o de Custos:")
print("=" * 50)
for key, value in cost_info.items():
    print(f"\n{key.upper()}:")
    if isinstance(value, dict):
        for k, v in value.items():
            print(f"  â€¢ {k}: {v}")
    elif isinstance(value, list):
        for item in value:
            print(f"  â€¢ {item}")
    else:
        print(f"  {value}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” VerificaÃ§Ã£o de Recursos DisponÃ­veis

# COMMAND ----------

# Verificar libraries disponÃ­veis
print("ğŸ“š Bibliotecas de Data Science DisponÃ­veis:")
libraries_to_check = [
    "pandas", "numpy", "matplotlib", "seaborn", 
    "scikit-learn", "mlflow", "koalas", "plotly"
]

available_libraries = []
for lib in libraries_to_check:
    try:
        __import__(lib)
        available_libraries.append(f"âœ… {lib}")
    except ImportError:
        available_libraries.append(f"âŒ {lib}")

for lib_status in available_libraries:
    print(lib_status)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ PrÃ³ximos Notebooks Recomendados
# MAGIC 
# MAGIC 1. **ğŸ“Š Data Analysis Sample** - AnÃ¡lise exploratÃ³ria de dados
# MAGIC 2. **ğŸ¤– ML Pipeline Example** - Pipeline completo de Machine Learning  
# MAGIC 3. **ğŸ’¾ Data Lake Integration** - IntegraÃ§Ã£o com Azure Data Lake
# MAGIC 4. **ğŸ“ˆ Visualization Examples** - Exemplos de visualizaÃ§Ã£o avanÃ§ada
# MAGIC 
# MAGIC ## ğŸ’¡ Dicas de OtimizaÃ§Ã£o
# MAGIC 
# MAGIC - ğŸ• **Auto-terminaÃ§Ã£o**: Cluster para automaticamente apÃ³s 20 min
# MAGIC - ğŸ’° **Spot Instances**: Economia de atÃ© 80% nos custos de compute
# MAGIC - ğŸ“Š **Monitoring**: Acompanhe custos via Azure Cost Management
# MAGIC - ğŸ”„ **Auto-scaling**: Recursos ajustam conforme demanda
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC **âœ… ConfiguraÃ§Ã£o verificada com sucesso!**
# MAGIC 
# MAGIC Seu ambiente Databricks estÃ¡ pronto para desenvolvimento com otimizaÃ§Ã£o de custos habilitada.